{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "qYX1n8RrCc-F"
   },
   "outputs": [],
   "source": [
    "!apt-get install openjdk-8-jdk-headless -qq > /dev/null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "4Fm3IrsLDBcB"
   },
   "outputs": [],
   "source": [
    "!wget -q https://dlcdn.apache.org/spark/spark-3.2.1/spark-3.2.1-bin-hadoop3.2.tgz\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kAowd2J-DSyN"
   },
   "outputs": [],
   "source": [
    "!tar xvzf spark-3.2.1-bin-hadoop3.2.tgz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "id": "dfdhcirkGDcj"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
    "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.2.1-bin-hadoop3.2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "G0-AdX-zFqSi",
    "outputId": "1598bba2-2b34-4b3a-c31c-f769ac13f12f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting findspark\n",
      "  Downloading findspark-2.0.0-py2.py3-none-any.whl (4.4 kB)\n",
      "Installing collected packages: findspark\n",
      "Successfully installed findspark-2.0.0\n"
     ]
    }
   ],
   "source": [
    "!pip install findspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HauZPaywJs98",
    "outputId": "57bfbab2-2673-4656-80e8-9b9e85402aea"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/spark-3.2.1-bin-hadoop3.2\n"
     ]
    }
   ],
   "source": [
    "!echo $SPARK_HOME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "SjDRZ0vTHbQT"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/home/al/spark-3.1.1-bin-hadoop3.2/jars/spark-unsafe_2.12-3.1.1.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n",
      "23/09/06 22:04:58 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "import findspark\n",
    "findspark.init('/home/al/spark-3.1.1-bin-hadoop3.2')\n",
    "\n",
    "import os\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkConf\n",
    "from pyspark import  SparkContext\n",
    "from pyspark.sql import functions as f\n",
    "appName = \"PySpark Task 1\"\n",
    "#master = \"spark://10.3.100.4:7077\"\n",
    "master = 'local[*]'\n",
    "# Create Spark session with Hive supported.\n",
    "spark = SparkSession.builder \\\n",
    "    .enableHiveSupport() \\\n",
    "    .appName(appName) \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- COUNTRY_ID: string (nullable = true)\n",
      " |-- COUNTRY_NAME: string (nullable = true)\n",
      " |-- REGION_ID: integer (nullable = true)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------+---------+\n",
      "|COUNTRY_ID|COUNTRY_NAME|REGION_ID|\n",
      "+----------+------------+---------+\n",
      "|        AR|   Argentina|        2|\n",
      "|        AU|   Australia|        3|\n",
      "+----------+------------+---------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#подгрузим все датасеты по порядку и выведем их схемы и первые пару строк\n",
    "df_countr = spark.read.orc(\"data_for_exam/countries\")\n",
    "df_countr.printSchema()\n",
    "df_countr.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- DEPARTMENT_ID: integer (nullable = true)\n",
      " |-- DEPARTMENT_NAME: string (nullable = true)\n",
      " |-- MANAGER_ID: integer (nullable = true)\n",
      " |-- LOCATION_ID: integer (nullable = true)\n",
      "\n",
      "+-------------+---------------+----------+-----------+\n",
      "|DEPARTMENT_ID|DEPARTMENT_NAME|MANAGER_ID|LOCATION_ID|\n",
      "+-------------+---------------+----------+-----------+\n",
      "|           10| Administration|       200|       1700|\n",
      "|           20|      Marketing|       201|       1800|\n",
      "+-------------+---------------+----------+-----------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_dep = spark.read\\\n",
    ".option(\"sep\", \",\")\\\n",
    ".option(\"header\", \"True\")\\\n",
    ".option(\"inferSchema\", True)\\\n",
    ".csv(\"data_for_exam/departments/departments\")\n",
    "df_dep.printSchema()\n",
    "df_dep.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- EMPLOYEE_ID: integer (nullable = true)\n",
      " |-- FIRST_NAME: string (nullable = true)\n",
      " |-- LAST_NAME: string (nullable = true)\n",
      " |-- EMAIL: string (nullable = true)\n",
      " |-- PHONE_NUMBER: string (nullable = true)\n",
      " |-- HIRE_DATE: string (nullable = true)\n",
      " |-- JOB_ID: string (nullable = true)\n",
      " |-- SALARY: integer (nullable = true)\n",
      " |-- COMMISSION_PCT: string (nullable = true)\n",
      " |-- MANAGER_ID: integer (nullable = true)\n",
      " |-- DEPARTMENT_ID: integer (nullable = true)\n",
      "\n",
      "+-----------+----------+---------+--------+------------+---------+-------+------+--------------+----------+-------------+\n",
      "|EMPLOYEE_ID|FIRST_NAME|LAST_NAME|   EMAIL|PHONE_NUMBER|HIRE_DATE| JOB_ID|SALARY|COMMISSION_PCT|MANAGER_ID|DEPARTMENT_ID|\n",
      "+-----------+----------+---------+--------+------------+---------+-------+------+--------------+----------+-------------+\n",
      "|        100|    Steven|     King|   SKING|515.123.4567| 17.06.03|AD_PRES| 24000|          null|      null|           90|\n",
      "|        101|     Neena|  Kochhar|NKOCHHAR|515.123.4568| 21.09.05|  AD_VP| 17000|          null|       100|           90|\n",
      "+-----------+----------+---------+--------+------------+---------+-------+------+--------------+----------+-------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_emp = spark.read\\\n",
    ".option(\"sep\", \"\\t\")\\\n",
    ".option(\"header\", \"True\")\\\n",
    ".option(\"inferSchema\", True)\\\n",
    ".csv(\"data_for_exam/employees/employees\")\n",
    "df_emp.printSchema()\n",
    "df_emp.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- department_id: long (nullable = true)\n",
      " |-- employee_id: long (nullable = true)\n",
      " |-- end_date: string (nullable = true)\n",
      " |-- job_id: string (nullable = true)\n",
      " |-- start_date: string (nullable = true)\n",
      "\n",
      "+-------------+-----------+--------+----------+----------+\n",
      "|department_id|employee_id|end_date|    job_id|start_date|\n",
      "+-------------+-----------+--------+----------+----------+\n",
      "|           60|        102|24.07.06|   IT_PROG|  13.01.01|\n",
      "|          110|        101|27.10.01|AC_ACCOUNT|  21.09.97|\n",
      "+-------------+-----------+--------+----------+----------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_jhis = spark.read.json(\"data_for_exam/job_history/job_history\")\n",
    "df_jhis.printSchema()\n",
    "df_jhis.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NpoFPaObPv-z",
    "outputId": "78609f72-c0f0-4819-ab7c-be1a42aa14b6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- JOB_ID: string (nullable = true)\n",
      " |-- JOB_TITLE: string (nullable = true)\n",
      " |-- MIN_SALARY: integer (nullable = true)\n",
      " |-- MAX_SALARY: integer (nullable = true)\n",
      "\n",
      "+-------+--------------------+----------+----------+\n",
      "| JOB_ID|           JOB_TITLE|MIN_SALARY|MAX_SALARY|\n",
      "+-------+--------------------+----------+----------+\n",
      "|AD_PRES|           President|     20080|     40000|\n",
      "|  AD_VP|Administration Vi...|     15000|     30000|\n",
      "+-------+--------------------+----------+----------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#df_emp = spark.read.json(\"/content/sample_data/anscombe.json\")\n",
    "df_jobs = spark.read\\\n",
    ".option(\"sep\", \";\")\\\n",
    ".option(\"header\", \"True\")\\\n",
    ".option(\"inferSchema\", True)\\\n",
    ".csv(\"data_for_exam/jobs/jobs\")\n",
    "df_jobs.printSchema()\n",
    "df_jobs.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- LOCATION_ID: integer (nullable = true)\n",
      " |-- STREET_ADDRESS: string (nullable = true)\n",
      " |-- POSTAL_CODE: string (nullable = true)\n",
      " |-- CITY: string (nullable = true)\n",
      " |-- STATE_PROVINCE: string (nullable = true)\n",
      " |-- COUNTRY_ID: string (nullable = true)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 13:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------------------+-----------+------+--------------+----------+\n",
      "|LOCATION_ID|      STREET_ADDRESS|POSTAL_CODE|  CITY|STATE_PROVINCE|COUNTRY_ID|\n",
      "+-----------+--------------------+-----------+------+--------------+----------+\n",
      "|       1000|1297 Via Cola di Rie|      00989|  Roma|          null|        IT|\n",
      "|       1100|93091 Calle della...|      10934|Venice|          null|        IT|\n",
      "+-----------+--------------------+-----------+------+--------------+----------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df_loc = spark.read.parquet(\"data_for_exam/locations\")\n",
    "df_loc.printSchema()\n",
    "df_loc.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_reg = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_emp.createOrReplaceTempView(\"employees\")\n",
    "df_countr.createOrReplaceTempView(\"countries\")\n",
    "df_loc.createOrReplaceTempView(\"locations\")\n",
    "df_dep.createOrReplaceTempView(\"departments\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "zkgVmsKGQGZS"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------------------------------------+\n",
      "|Dream_Salaries                                                  |\n",
      "+----------------------------------------------------------------+\n",
      "|King зарабатывает 24000 каждый месяц, но хочет получать 72000   |\n",
      "|Kochhar зарабатывает 17000 каждый месяц, но хочет получать 51000|\n",
      "|De Haan зарабатывает 17000 каждый месяц, но хочет получать 51000|\n",
      "|Hunold зарабатывает 9000 каждый месяц, но хочет получать 27000  |\n",
      "|Ernst зарабатывает 6000 каждый месяц, но хочет получать 18000   |\n",
      "+----------------------------------------------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#1\n",
    "df = spark.sql(\"\"\"SELECT last_name || ' зарабатывает ' || salary || ' каждый месяц, но хочет получать ' || salary*3\n",
    "                  AS Dream_Salaries\n",
    "                  FROM employees\"\"\")\n",
    "\n",
    "df.write\\\n",
    ".format(\"parquet\")\\\n",
    ".option(\"compression\", \"gzip\")\\\n",
    ".mode(\"overwrite\")\\\n",
    ".save(\"results/task1\")\n",
    "\n",
    "spark.read.format(\"parquet\")\\\n",
    ".option(\"compression\", \"gzip\")\\\n",
    ".load(\"results/task1\")\\\n",
    ".show(5, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----------------------+---------+----------------+------------------------+\n",
      "|location_id|street_address         |city     |state_province  |country_name            |\n",
      "+-----------+-----------------------+---------+----------------+------------------------+\n",
      "|1000       |1297 Via Cola di Rie   |Roma     |null            |Italy                   |\n",
      "|1100       |93091 Calle della Testa|Venice   |null            |Italy                   |\n",
      "|1200       |2017 Shinjuku-ku       |Tokyo    |Tokyo Prefecture|Japan                   |\n",
      "|1300       |9450 Kamiya-cho        |Hiroshima|null            |Japan                   |\n",
      "|1400       |2014 Jabberwocky Rd    |Southlake|Texas           |United States of America|\n",
      "+-----------+-----------------------+---------+----------------+------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#2\n",
    "df = spark.sql(\"\"\"SELECT location_id, street_address, city, state_province, country_name\n",
    "                  FROM locations\n",
    "                  JOIN countries ON locations.country_id = countries.country_id\"\"\")\n",
    "df.write\\\n",
    ".format(\"parquet\")\\\n",
    ".option(\"compression\", \"gzip\")\\\n",
    ".mode(\"overwrite\")\\\n",
    ".save(\"results/task2\")\n",
    "\n",
    "spark.read.format(\"parquet\")\\\n",
    ".option(\"compression\", \"gzip\")\\\n",
    ".load(\"results/task2\")\\\n",
    ".show(5, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-------------+---------------+\n",
      "|last_name|department_id|department_name|\n",
      "+---------+-------------+---------------+\n",
      "|     King|           90|      Executive|\n",
      "|  Kochhar|           90|      Executive|\n",
      "|  De Haan|           90|      Executive|\n",
      "|   Hunold|           60|             IT|\n",
      "|    Ernst|           60|             IT|\n",
      "+---------+-------------+---------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#3\n",
    "df = spark.sql(\"\"\"SELECT last_name, departments.department_id, department_name\n",
    "                  FROM employees\n",
    "                  JOIN departments ON employees.department_id = departments.department_id\"\"\")\n",
    "df.show(5)\n",
    "\n",
    "#сохранить в формате avro со сжатием gzip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---+-------+----+\n",
      "|Employee|EMP|Manager| Mgr|\n",
      "+--------+---+-------+----+\n",
      "|    King|100|   null|null|\n",
      "| Kochhar|101|   King| 100|\n",
      "| De Haan|102|   King| 100|\n",
      "|  Hunold|103|De Haan| 102|\n",
      "|   Ernst|104| Hunold| 103|\n",
      "+--------+---+-------+----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#4\n",
    "df = spark.sql(\"\"\"SELECT empl.last_name AS Employee,\n",
    "                empl.employee_id AS EMP,\n",
    "                chief.last_name AS Manager,\n",
    "                empl.manager_id AS Mgr\n",
    "                FROM employees AS empl\n",
    "                LEFT JOIN employees AS chief ON chief.employee_id = empl.manager_id\"\"\")\n",
    "df.show(5)\n",
    "\n",
    "#Результат сохранить в формате avro со сжатием Snappy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------+\n",
      "|last_name|salary|\n",
      "+---------+------+\n",
      "|Kochhar  |17000 |\n",
      "|De Haan  |17000 |\n",
      "|Raphaely |11000 |\n",
      "|Weiss    |8000  |\n",
      "|Fripp    |8200  |\n",
      "+---------+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#5\n",
    "df = spark.sql(\"\"\"SELECT last_name, salary\n",
    "                FROM employees\n",
    "                WHERE manager_id in (SELECT employee_id\n",
    "                                    FROM employees\n",
    "                                    WHERE last_name = 'King')\"\"\")\n",
    "df.write\\\n",
    ".format(\"parquet\")\\\n",
    ".option(\"compression\", \"gzip\")\\\n",
    ".mode(\"overwrite\")\\\n",
    ".save(\"results/task5\")\n",
    "\n",
    "spark.read.format(\"parquet\")\\\n",
    ".option(\"compression\", \"gzip\")\\\n",
    ".load(\"results/task5\")\\\n",
    ".show(5, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|last_name|\n",
      "+---------+\n",
      "|King     |\n",
      "|Kochhar  |\n",
      "|De Haan  |\n",
      "|Greenberg|\n",
      "|Raphaely |\n",
      "+---------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#6\n",
    "df = spark.sql(\"\"\"SELECT last_name\n",
    "                FROM employees\n",
    "                WHERE salary > (SELECT max(salary)\n",
    "                                FROM employees\n",
    "                                WHERE department_id = 60)\"\"\")\n",
    "df.write\\\n",
    ".format(\"parquet\")\\\n",
    ".option(\"compression\", \"gzip\")\\\n",
    ".mode(\"overwrite\")\\\n",
    ".save(\"results/task6\")\n",
    "\n",
    "spark.read.format(\"parquet\")\\\n",
    ".option(\"compression\", \"gzip\")\\\n",
    ".load(\"results/task6\")\\\n",
    ".show(5, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+---------+------+\n",
      "|employee_id|last_name|salary|\n",
      "+-----------+---------+------+\n",
      "|103        |Hunold   |9000  |\n",
      "|108        |Greenberg|12008 |\n",
      "|109        |Faviet   |9000  |\n",
      "|110        |Chen     |8200  |\n",
      "|111        |Sciarra  |7700  |\n",
      "+-----------+---------+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#7\n",
    "df = spark.sql(\"\"\"SELECT employee_id, last_name, salary\n",
    "                FROM employees\n",
    "                WHERE department_id in (SELECT department_id\n",
    "                                        FROM employees\n",
    "                                        WHERE upper(last_name) LIKE '%U%')\n",
    "                AND salary > (SELECT avg(salary)\n",
    "                              FROM employees)\"\"\")\n",
    "df.write\\\n",
    ".format(\"parquet\")\\\n",
    ".option(\"compression\", \"gzip\")\\\n",
    ".mode(\"overwrite\")\\\n",
    ".save(\"results/task7\")\n",
    "\n",
    "spark.read.format(\"parquet\")\\\n",
    ".option(\"compression\", \"gzip\")\\\n",
    ".load(\"results/task7\")\\\n",
    ".show(5, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-------------+--------------------+\n",
      "|last_name|department_id|department_name     |\n",
      "+---------+-------------+--------------------+\n",
      "|null     |null         |Shareholder Services|\n",
      "|null     |null         |Control And Credit  |\n",
      "|null     |null         |Government Sales    |\n",
      "|null     |null         |Corporate Tax       |\n",
      "|null     |null         |Manufacturing       |\n",
      "+---------+-------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#8\n",
    "df = spark.sql(\"\"\"\n",
    "SELECT last_name, employees.department_id, department_name\n",
    "FROM employees\n",
    "LEFT JOIN departments\n",
    "        ON employees.department_id = departments.department_id\n",
    "WHERE employees.department_id IS NULL\n",
    "    UNION\n",
    "SELECT last_name, employees.department_id, department_name\n",
    "FROM departments\n",
    "LEFT JOIN employees\n",
    "        ON employees.department_id = departments.department_id\n",
    "WHERE employees.last_name IS NULL\"\"\")\n",
    "\n",
    "df.write\\\n",
    ".format(\"parquet\")\\\n",
    ".option(\"compression\", \"gzip\")\\\n",
    ".mode(\"overwrite\")\\\n",
    ".save(\"results/task8\")\n",
    "\n",
    "spark.read.format(\"parquet\")\\\n",
    ".option(\"compression\", \"gzip\")\\\n",
    ".load(\"results/task8\")\\\n",
    ".show(5, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "Untitled0.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
